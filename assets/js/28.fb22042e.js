(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{356:function(t,a,s){"use strict";s.r(a);var e=s(19),i=Object(e.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"特征变换"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#特征变换"}},[t._v("#")]),t._v(" 特征变换")]),t._v(" "),s("h2",{attrs:{id:"引言"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#引言"}},[t._v("#")]),t._v(" 引言")]),t._v(" "),s("p",[t._v("维数灾难：在样本量一定的情况下，维数越高，样本在空间中的分布越呈稀疏性。")]),t._v(" "),s("ul",[s("li",[t._v("1维空间，10个平均分布的点可以将单位区间按0.1采样")]),t._v(" "),s("li",[t._v("2维空间，100个平均分布的点可以将单位区间按0.1采样")]),t._v(" "),s("li",[t._v("N维空间，10^n个平均分布的点。。。")])]),t._v(" "),s("p",[t._v("本征维度：数据本征内蕴的表示维度。")]),t._v(" "),s("ul",[s("li",[t._v("任意低维数据空间可简单地通过增加空余（如复制）或随 机维将其转换至更高维空间中，而没有携带更多信息；")]),t._v(" "),s("li",[t._v("许多高维空间中的数据集也可削减至低维空间数据，而不必丢失重要信息")])]),t._v(" "),s("p",[t._v("特征变换：从一组已有特征进行变换，得到新特征的过程")]),t._v(" "),s("ul",[s("li",[t._v("降低特征空间的维度，缓解“维数灾难”，减少计算量")]),t._v(" "),s("li",[t._v("减少特征之间可能存在的相关性，降低分类器学习难度")]),t._v(" "),s("li",[t._v("处理高维数据的两大主流技术之一")])]),t._v(" "),s("blockquote",[s("p",[t._v("线性特征变换（子空间分析）：采用线性变换关系将原特征变换至一个新的空间（通常维度更低），PCA、LDA")]),t._v(" "),s("p",[t._v("非线性特征变换：采用非线性变换关系将原特征变换至一 个新的空间（通常性能更好），KPCA、KLDA")])]),t._v(" "),s("h2",{attrs:{id:"主成分分析（pca）"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#主成分分析（pca）"}},[t._v("#")]),t._v(" 主成分分析（PCA）")]),t._v(" "),s("p",[t._v("无需样本类别标签，非监督学习方法。使得变换后低维空间最大程度保持原空间特性。")]),t._v(" "),s("blockquote",[s("p",[t._v("历史：")]),t._v(" "),s("ul",[s("li",[t._v("1901年Karl Pearson（统计学之父）首次提出")]),t._v(" "),s("li",[t._v("Harold Hotelling于1933年加于发展")]),t._v(" "),s("li",[t._v("Kari Karhunen和Michel Loeve提出了KarhunenLoeve理论")]),t._v(" "),s("li",[t._v("Turk和Pentland于1991年提出eigenfaces，用于人脸识别，开启子空间分析的研究先河")])])]),t._v(" "),s("p",[t._v("作用：将原始的高维数据变换至一个低维的特征空间， 以最大程度保持原始数据的样本差异信息，在低维特征 空间进行分析，降低问题复杂度、减少数据噪声影响。")]),t._v(" "),s("h3",{attrs:{id:"算法过程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#算法过程"}},[t._v("#")]),t._v(" 算法过程")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126213738623.png",alt:"image-20201126213738623"}}),t._v(" "),s("p",[t._v("目的：使用较少的新特征来重新表示原始的特征。")]),t._v(" "),s("ol",[s("li",[t._v("每个新特征都是原始特征的线性组合；")]),t._v(" "),s("li",[t._v("新特征按照重要性从大到小排序；")]),t._v(" "),s("li",[t._v("新特征之间相互不相关。")])]),t._v(" "),s("p",[t._v("解释：主成分的选择")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126223958452.png",alt:"image-20201126223958452"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224028351.png",alt:"image-20201126224028351"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224048358.png",alt:"image-20201126224048358"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224112187.png",alt:"image-20201126224112187"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224129189.png",alt:"image-20201126224129189"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224142480.png",alt:"image-20201126224142480"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224238123.png",alt:"image-20201126224238123"}}),t._v(" "),s("h3",{attrs:{id:"应用"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#应用"}},[t._v("#")]),t._v(" 应用")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("降维：直接对所有数据使用PCA，便于数据分析， 如聚类、分类、可视化等。")])]),t._v(" "),s("li",[s("p",[t._v("数据去噪：对数据使用PCA，将协方差矩阵Σ特征 值很小的成分置为0（取特征值较大的前k个主成分）， 反变换得到去噪后原始数据。")])]),t._v(" "),s("li",[s("p",[t._v("人脸识别（eigenfaces）：使用PCA对人脸图像进 行特征变换，再利用kNN在特征空间进行人脸识别。")]),t._v(" "),s("blockquote",[s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224323494.png",alt:"image-20201126224323494"}}),t._v(" "),s("p",[t._v("特征值分解技巧：")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224351954.png",alt:"image-20201126224351954"}}),t._v(" "),s("p",[t._v("即可以利用"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("msup",[s("mi",[t._v("X")]),s("mi",[t._v("T")])],1),s("mi",[t._v("X")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("X^TX")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.8413309999999999em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"0.8413309999999999em","vertical-align":"0em"}}),s("span",{staticClass:"base textstyle uncramped"},[s("span",{staticClass:"mord"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),s("span",{staticClass:"vlist"},[s("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),s("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])]),s("span",{staticClass:"baseline-fix"},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])]),s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")])])])]),t._v("的特征值和特征向量V，来表达"),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",[s("semantics",[s("mrow",[s("mi",[t._v("X")]),s("msup",[s("mi",[t._v("X")]),s("mi",[t._v("T")])],1)],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("XX^T")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"strut",staticStyle:{height:"0.8413309999999999em"}}),s("span",{staticClass:"strut bottom",staticStyle:{height:"0.8413309999999999em","vertical-align":"0em"}}),s("span",{staticClass:"base textstyle uncramped"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),s("span",{staticClass:"mord"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.07847em"}},[t._v("X")]),s("span",{staticClass:"vlist"},[s("span",{staticStyle:{top:"-0.363em","margin-right":"0.05em"}},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),s("span",{staticClass:"reset-textstyle scriptstyle uncramped"},[s("span",{staticClass:"mord mathit",staticStyle:{"margin-right":"0.13889em"}},[t._v("T")])])]),s("span",{staticClass:"baseline-fix"},[s("span",{staticClass:"fontsize-ensurer reset-size5 size5"},[s("span",{staticStyle:{"font-size":"0em"}},[t._v("​")])]),t._v("​")])])])])])]),t._v("的特征向量。")])])])]),t._v(" "),s("h2",{attrs:{id:"核主成分分析（kpca）"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#核主成分分析（kpca）"}},[t._v("#")]),t._v(" 核主成分分析（KPCA）")]),t._v(" "),s("p",[t._v("基本思想：对样本进行非线性变换，在变换空间进 行主成分分析。等价于在原始空间进行非线性主成分分析。")]),t._v(" "),s("p",[t._v("核心技术：核技巧，绕过复杂的非线性变换环节。")]),t._v(" "),s("p",[t._v("问题：")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224615567.png",alt:"image-20201126224615567"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224638442.png",alt:"image-20201126224638442"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126224657403.png",alt:"image-20201126224657403"}}),t._v(" "),s("h2",{attrs:{id:"线性判别分析"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#线性判别分析"}},[t._v("#")]),t._v(" 线性判别分析")]),t._v(" "),s("p",[t._v("需要样本类别标签，监督学习方法。使得变换后低维空间利于分类。")]),t._v(" "),s("p",[t._v("线性判别分析，又称Fisher判别分析，是一种监督 的子空间分析方法。")]),t._v(" "),s("blockquote",[s("p",[t._v("采用Fisher判别准则，进行特征变换：寻找线性变 换矩阵，使得变换后特征的类内散度尽可能小， 同时类间散度尽可能大。")])]),t._v(" "),s("p",[t._v("作用：将原始的高维数据变换至一个低维的特征空间， 以最大化数据的线性可分性，在低维特征空间进行分析， 提高分类性能。")]),t._v(" "),s("p",[t._v("原理：")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126225126727.png",alt:"image-20201126225126727"}}),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126225138521.png",alt:"image-20201126225138521"}}),t._v(" "),s("h3",{attrs:{id:"算法流程"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#算法流程"}},[t._v("#")]),t._v(" 算法流程")]),t._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:"10feature_transformation.assets/image-20201126225333329.png",alt:"image-20201126225333329"}}),t._v(" "),s("h3",{attrs:{id:"应用-2"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#应用-2"}},[t._v("#")]),t._v(" 应用")]),t._v(" "),s("ul",[s("li",[t._v("LDA方法可以和PCA相结合使用\n"),s("ol",[s("li",[t._v("使用PCA对原始数据降维")]),t._v(" "),s("li",[t._v("对降维后数据，应用LDA，变换至最大线性可分特征空间")]),t._v(" "),s("li",[t._v("使用kNN进行分类")])])])])])}),[],!1,null,null,null);a.default=i.exports}}]);